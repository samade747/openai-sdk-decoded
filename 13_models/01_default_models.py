"""
01_default_models.py

This module covers the default OpenAI model types in the OpenAI Agents SDK.
Focus: Understanding ResponsesModel vs ChatCompletionsModel and when to use each.

Learning Objectives:
- Understand the two OpenAI model types available
- Learn when to use ResponsesModel (recommended) vs ChatCompletionsModel
- See practical differences in feature support
- Understand the transition from Chat Completions to Responses API

Key Concepts:
- OpenAIResponsesModel: Uses new Responses API (recommended)
- OpenAIChatCompletionsModel: Uses traditional Chat Completions API
- Feature differences and capabilities
- Migration patterns and best practices

Based on: https://openai.github.io/openai-agents-python/models/
"""

import asyncio
from agents import Agent, OpenAIChatCompletionsModel
from openai import AsyncOpenAI

# ================== DEFAULT MODELS FUNDAMENTALS ==================


async def demo_responses_model():
    """Demonstrate the recommended OpenAIResponsesModel."""
    print("=== Demo: OpenAI Responses Model (Recommended) ===")

    print("ğŸ¯ OpenAIResponsesModel Overview:")
    print("   â€¢ Uses the new OpenAI Responses API")
    print("   â€¢ Recommended by OpenAI for new applications")
    print("   â€¢ Enhanced features and capabilities")
    print("   â€¢ Better structured outputs support")
    print("   â€¢ Improved function calling")
    print()

    print("âœ¨ Key Features of Responses API:")
    print("   âœ“ Advanced structured outputs")
    print("   âœ“ Enhanced function calling")
    print("   âœ“ Better response formatting")
    print("   âœ“ Improved context handling")
    print("   âœ“ Future-proof API design")
    print()

    try:
        # Create agent using default (Responses) model
        responses_agent = Agent(
            name="ResponsesAgent",
            instructions="You demonstrate the Responses API model. Respond clearly and concisely."
        )

        print("ğŸ¤– Creating agent with default OpenAIResponsesModel...")
        print("   â€¢ Default model type: OpenAIResponsesModel")
        print("   â€¢ Uses: Responses API endpoint")
        print("   â€¢ Features: All latest capabilities enabled")

        # Note: This would use the Responses API if API key is available
        print(f"\nğŸ“Š Agent Configuration:")
        print(f"   Model Type: {type(responses_agent.model).__name__}")
        print(f"   API Endpoint: Responses API")
        print(f"   Capabilities: Enhanced structured outputs, function calling")

        print(f"\nğŸ’¡ When to Use ResponsesModel:")
        print(f"   âœ“ New applications and projects")
        print(f"   âœ“ When you need structured outputs")
        print(f"   âœ“ Advanced function calling scenarios")
        print(f"   âœ“ Future-proofing your application")

    except Exception as e:
        print(f"âŒ Configuration error: {e}")
        print(f"   This is expected without proper API setup")


async def demo_chat_completions_model():
    """Demonstrate the traditional OpenAIChatCompletionsModel."""
    print("\n=== Demo: OpenAI Chat Completions Model ===")

    print("ğŸ”„ OpenAIChatCompletionsModel Overview:")
    print("   â€¢ Uses traditional Chat Completions API")
    print("   â€¢ Widely supported across LLM providers")
    print("   â€¢ Stable and well-established")
    print("   â€¢ Maximum compatibility")
    print("   â€¢ Legacy applications support")
    print()

    print("ğŸ› ï¸ Key Characteristics:")
    print("   âœ“ Universal provider compatibility")
    print("   âœ“ Stable API interface")
    print("   âœ“ Proven reliability")
    print("   âœ“ Extensive ecosystem support")
    print("   âœ“ Backward compatibility")
    print()

    try:
        # Create agent explicitly using Chat Completions model
        chat_completions_agent = Agent(
            name="ChatCompletionsAgent",
            instructions="You demonstrate the Chat Completions API model.",
            model=OpenAIChatCompletionsModel(
                model="gpt-4o",
                openai_client=AsyncOpenAI()
            )
        )

        print("ğŸ¤– Creating agent with explicit OpenAIChatCompletionsModel...")
        print("   â€¢ Model type: OpenAIChatCompletionsModel")
        print("   â€¢ Uses: Chat Completions API endpoint")
        print("   â€¢ Features: Traditional capabilities")

        print(f"\nğŸ“Š Agent Configuration:")
        print(f"   Model Type: {type(chat_completions_agent.model).__name__}")
        print(f"   API Endpoint: Chat Completions API")
        print(f"   Compatibility: Maximum provider support")

        print(f"\nğŸ’¡ When to Use ChatCompletionsModel:")
        print(f"   âœ“ Legacy application compatibility")
        print(f"   âœ“ Multi-provider support needed")
        print(f"   âœ“ Proven stability requirements")
        print(f"   âœ“ Existing infrastructure integration")

    except Exception as e:
        print(f"âŒ Configuration error: {e}")
        print(f"   This demonstrates configuration patterns")


async def demo_model_comparison():
    """Compare the two model types side by side."""
    print("\n=== Demo: Model Type Comparison ===")

    print("âš–ï¸ ResponsesModel vs ChatCompletionsModel:")
    print()

    comparison_table = {
        "Feature": ["API Endpoint", "Structured Outputs", "Function Calling", "Provider Support", "Future Updates", "Stability", "Performance", "Compatibility"],
        "ResponsesModel": [
            "Responses API",
            "Advanced âœ“âœ“",
            "Enhanced âœ“âœ“",
            "OpenAI only",
            "Latest features âœ“âœ“",
            "New (evolving)",
            "Optimized âœ“âœ“",
            "OpenAI specific"
        ],
        "ChatCompletionsModel": [
            "Chat Completions API",
            "Basic âœ“",
            "Standard âœ“",
            "Universal âœ“âœ“",
            "Stable updates",
            "Very stable âœ“âœ“",
            "Standard âœ“",
            "Universal âœ“âœ“"
        ]
    }

    # Print comparison table
    for i, feature in enumerate(comparison_table["Feature"]):
        print(
            f"   {feature:20} | {comparison_table['ResponsesModel'][i]:20} | {comparison_table['ChatCompletionsModel'][i]}")

    print(f"\nğŸ¯ Decision Matrix:")
    print(f"   Choose ResponsesModel when:")
    print(f"   âœ“ Building new OpenAI-specific applications")
    print(f"   âœ“ Need advanced structured outputs")
    print(f"   âœ“ Want latest features and performance")
    print(f"   âœ“ Future-proofing is important")
    print()
    print(f"   Choose ChatCompletionsModel when:")
    print(f"   âœ“ Need multi-provider compatibility")
    print(f"   âœ“ Have existing legacy integrations")
    print(f"   âœ“ Stability is more important than features")
    print(f"   âœ“ Working with non-OpenAI providers")


async def demo_default_behavior():
    """Demonstrate the default model selection behavior."""
    print("\n=== Demo: Default Model Selection ===")

    print("ğŸ”§ SDK Default Behavior:")
    print("   â€¢ ResponsesModel is used by default")
    print("   â€¢ No explicit model configuration needed")
    print("   â€¢ Automatic selection of optimal API")
    print("   â€¢ Best practice for new applications")
    print()

    try:
        # Default agent (uses ResponsesModel automatically)
        default_agent = Agent(
            name="DefaultAgent",
            instructions="You use the default model configuration."
        )

        print("ğŸ¤– Default Agent Configuration:")
        print(f"   Model Class: {type(default_agent.model).__name__}")
        print(f"   Selection: Automatic (SDK default)")
        print(f"   API: Responses API (recommended)")

        print(f"\nâš™ï¸ Configuration Patterns:")

        # Pattern 1: Explicit default (same as implicit)
        print(f"   Pattern 1 - Implicit Default:")
        print(f"   Agent(name='Agent', instructions='...')")
        print(f"   â†’ Uses: OpenAIResponsesModel")

        # Pattern 2: Explicit model specification
        print(f"\n   Pattern 2 - Explicit Responses:")
        print(f"   Agent(model=OpenAIResponsesModel(model='gpt-4o'))")
        print(f"   â†’ Uses: OpenAIResponsesModel (explicit)")

        # Pattern 3: Chat Completions override
        print(f"\n   Pattern 3 - Chat Completions Override:")
        print(f"   Agent(model=OpenAIChatCompletionsModel(...))")
        print(f"   â†’ Uses: OpenAIChatCompletionsModel (explicit)")

        print(f"\nğŸ’¡ Best Practice Recommendations:")
        print(f"   âœ“ Use default (ResponsesModel) for new projects")
        print(f"   âœ“ Explicitly specify only when needed")
        print(f"   âœ“ Document model choice reasoning")
        print(f"   âœ“ Consider migration path for legacy apps")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def demo_feature_support():
    """Demonstrate feature differences between model types."""
    print("\n=== Demo: Feature Support Differences ===")

    print("ğŸŒŸ Feature Support Matrix:")
    print()

    features = {
        "Basic Chat": {"Responses": "âœ“ Supported", "ChatCompletions": "âœ“ Supported"},
        "Function Calling": {"Responses": "âœ“ Enhanced", "ChatCompletions": "âœ“ Standard"},
        "Structured Outputs": {"Responses": "âœ“ Advanced", "ChatCompletions": "âœ“ Basic"},
        "JSON Schema": {"Responses": "âœ“ Full support", "ChatCompletions": "âœ“ Limited"},
        "Tool Use": {"Responses": "âœ“ Optimized", "ChatCompletions": "âœ“ Standard"},
        "Streaming": {"Responses": "âœ“ Enhanced", "ChatCompletions": "âœ“ Standard"},
        "Context Management": {"Responses": "âœ“ Advanced", "ChatCompletions": "âœ“ Basic"},
        "Multi-turn": {"Responses": "âœ“ Optimized", "ChatCompletions": "âœ“ Standard"},
        "Provider Portability": {"Responses": "âŒ OpenAI only", "ChatCompletions": "âœ“ Universal"},
        "Legacy Support": {"Responses": "âŒ New only", "ChatCompletions": "âœ“ Full"}
    }

    for feature, support in features.items():
        print(
            f"   {feature:20} | {support['Responses']:15} | {support['ChatCompletions']}")

    print(f"\nğŸ¯ Feature Selection Guide:")
    print(f"   For Advanced AI Applications:")
    print(f"   â†’ Choose ResponsesModel for enhanced capabilities")
    print(f"   For Maximum Compatibility:")
    print(f"   â†’ Choose ChatCompletionsModel for broad support")
    print(f"   For Future-Proofing:")
    print(f"   â†’ Choose ResponsesModel for latest developments")
    print(f"   For Legacy Integration:")
    print(f"   â†’ Choose ChatCompletionsModel for compatibility")


async def demo_migration_patterns():
    """Demonstrate migration from Chat Completions to Responses."""
    print("\n=== Demo: Migration Patterns ===")

    print("ğŸ”„ Migration from Chat Completions to Responses:")
    print()

    print("ğŸ“‹ Migration Checklist:")
    migration_steps = [
        "Assess current Chat Completions usage",
        "Identify advanced features needed",
        "Test Responses API compatibility",
        "Update model configuration",
        "Verify functionality works",
        "Monitor performance improvements",
        "Complete migration rollout"
    ]

    for i, step in enumerate(migration_steps, 1):
        print(f"   {i}. {step}")

    print(f"\nâš™ï¸ Migration Code Patterns:")

    print(f"\n   Before (Chat Completions):")
    print(f"   ```python")
    print(f"   agent = Agent(")
    print(f"       model=OpenAIChatCompletionsModel(")
    print(f"           model='gpt-4o',")
    print(f"           openai_client=AsyncOpenAI()")
    print(f"       )")
    print(f"   )")
    print(f"   ```")

    print(f"\n   After (Responses - Default):")
    print(f"   ```python")
    print(f"   agent = Agent(")
    print(f"       # Uses ResponsesModel by default")
    print(f"       model='gpt-4o'  # or omit for default")
    print(f"   )")
    print(f"   ```")

    print(f"\n   After (Responses - Explicit):")
    print(f"   ```python")
    print(f"   agent = Agent(")
    print(f"       model=OpenAIResponsesModel(")
    print(f"           model='gpt-4o'")
    print(f"       )")
    print(f"   )")
    print(f"   ```")

    print(f"\nğŸ¯ Migration Benefits:")
    print(f"   âœ“ Access to latest OpenAI features")
    print(f"   âœ“ Improved performance and reliability")
    print(f"   âœ“ Enhanced structured outputs")
    print(f"   âœ“ Better function calling")
    print(f"   âœ“ Future feature compatibility")

    print(f"\nâš ï¸ Migration Considerations:")
    print(f"   â€¢ Test thoroughly in staging environment")
    print(f"   â€¢ Verify all existing functionality works")
    print(f"   â€¢ Plan for any API differences")
    print(f"   â€¢ Monitor performance after migration")
    print(f"   â€¢ Have rollback plan ready")


# ================== MAIN EXECUTION ==================


async def main():
    """Run all default models demonstrations."""
    print("ğŸ¤– OpenAI Agents SDK - Default Models ğŸ¤–")
    print("\nThis module covers the two OpenAI model types and when to use each.")
    print("Focus: Understanding ResponsesModel vs ChatCompletionsModel\n")

    # Run all demonstrations
    await demo_responses_model()
    await demo_chat_completions_model()
    await demo_model_comparison()
    await demo_default_behavior()
    await demo_feature_support()
    await demo_migration_patterns()

    print("\n" + "="*60)
    print("ğŸ“ Key Takeaways - Default Models:")
    print("â€¢ ResponsesModel: Recommended for new OpenAI applications")
    print("â€¢ ChatCompletionsModel: Best for multi-provider compatibility")
    print("â€¢ Default behavior uses ResponsesModel automatically")
    print("â€¢ Feature support varies between model types")
    print("â€¢ Migration to ResponsesModel provides enhanced capabilities")

    print(f"\nğŸ¤– Model Selection Summary:")
    print(f"   ğŸ¯ ResponsesModel: Latest features, OpenAI-specific")
    print(f"   ğŸ”„ ChatCompletionsModel: Universal compatibility")
    print(f"   âš™ï¸ Default: ResponsesModel (recommended)")

    print(f"\nğŸ¯ Next Steps:")
    print(f"â€¢ Learn LiteLLM integration in 02_litellm_integration.py")
    print(f"â€¢ Understand model configuration in 03_model_configuration.py")


if __name__ == "__main__":
    asyncio.run(main())
